{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport requests\nimport zipfile\nfrom bs4 import BeautifulSoup\n\n# Define URLs\nBASE_URL = \"https://vbpl.vn/TW/Pages/vbpq-toanvan.aspx?ItemID={}\"\nPROPERTY_URL = \"https://vbpl.vn/tw/Pages/vbpq-thuoctinh.aspx?dvid=13&ItemID={}\"\nHISTORY_URL = \"https://vbpl.vn/tw/Pages/vbpq-lichsu.aspx?dvid=13&ItemID={}\"\nRELATED_URL = \"https://vbpl.vn/TW/Pages/vbpq-vanbanlienquan.aspx?ItemID={}\"\nPDF_URL = \"https://vbpl.vn/tw/Pages/vbpq-van-ban-goc.aspx?ItemID={}\"\n\n# Download and unzip dataset\nZIP_URL = \"https://phapdien.moj.gov.vn/TraCuuPhapDien/Files/BoPhapDienDienTu.zip\"\nZIP_PATH = \"BoPhapDienDienTu.zip\"\nEXTRACT_PATH = \"BoPhapDienDienTu\"\n\nif not os.path.exists(EXTRACT_PATH):\n    print(\"Downloading dataset...\")\n    response = requests.get(ZIP_URL, stream=True)\n    with open(ZIP_PATH, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n\n    print(\"Extracting dataset...\")\n    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n        zip_ref.extractall(EXTRACT_PATH)\n    os.remove(ZIP_PATH)\n\n# Create necessary directories\nfolders = [\"vbpl\", \"property\", \"history\", \"related\", \"pdf\"]\nfor folder in folders:\n    os.makedirs(os.path.join(EXTRACT_PATH, folder), exist_ok=True)\n\n# Parse index files in \"demuc\" directory\ndemuc_path = os.path.join(EXTRACT_PATH, \"demuc\")\nindex_files = [f for f in os.listdir(demuc_path) if f.endswith(\".html\")]\n\ndef save_page(url, save_path):\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(save_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(response.text)\n    except Exception as e:\n        print(f\"Failed to download {url}: {e}\")\n\n# Extract document links and save pages\nfor index_file in index_files:\n    index_path = os.path.join(demuc_path, index_file)\n    with open(index_path, \"r\", encoding=\"utf-8\") as file:\n        soup = BeautifulSoup(file, \"html.parser\")\n\n        for link in soup.find_all(\"a\", href=True):\n            if \"ItemID\" in link[\"href\"]:\n                item_id = link[\"href\"].split(\"ItemID=\")[1].split(\"&\")[0]\n\n                # Save different pages\n                save_page(BASE_URL.format(item_id), os.path.join(EXTRACT_PATH, \"vbpl\", f\"full_{item_id}.html\"))\n                save_page(PROPERTY_URL.format(item_id), os.path.join(EXTRACT_PATH, \"property\", f\"p_{item_id}.html\"))\n                save_page(HISTORY_URL.format(item_id), os.path.join(EXTRACT_PATH, \"history\", f\"h_{item_id}.html\"))\n                save_page(RELATED_URL.format(item_id), os.path.join(EXTRACT_PATH, \"related\", f\"r_{item_id}.html\"))\n                save_page(PDF_URL.format(item_id), os.path.join(EXTRACT_PATH, \"pdf\", f\"pdf_{item_id}.html\"))\n\nprint(\"Crawling complete!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}